{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from os import path\n",
    "from scipy.cluster.vq import vq\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(os.getcwd(),'data/HMP_Dataset')\n",
    "filelist = [os.path.join(DATA_PATH,f) for f in os.listdir('data/HMP_Dataset') if (not f.endswith(\".txt\") and (not f.endswith(\".m\")))] # I put my data in this folder\n",
    "def generate_segments(data,time_unit):\n",
    "    no_of_rows = np.shape(data)[0]  \n",
    "    mod = no_of_rows % time_unit  \n",
    "    if mod != 0:\n",
    "        data_to_segment = np.array(data)[:-mod, :]  \n",
    "    else:\n",
    "        data_to_segment = np.array(data)\n",
    "    vector_segment = data_to_segment.reshape(int(no_of_rows / time_unit),\n",
    "                                             time_unit * 3)\n",
    "    return pd.DataFrame(vector_segment)\n",
    "\n",
    "def read_attribute_from_all_file(dir,time_unit):\n",
    "    files = os.listdir(dir) \n",
    "    full_data_train = pd.DataFrame() \n",
    "    for file in files: \n",
    "        file_path = os.path.join(dir, file)  \n",
    "        data = pd.read_csv(file_path, sep=\" \", index_col=None, names=['x', 'y', 'z'],\n",
    "                           skip_blank_lines=True).dropna() \n",
    "        segmented_data_train = generate_segments(data,\n",
    "                                                 time_unit) \n",
    "        full_data_train = full_data_train.append(segmented_data_train,\n",
    "                                                 ignore_index=True) \n",
    "    return full_data_train\n",
    "def generate_vectors(n_cluster, time_unit):\n",
    "    feature_vector = []\n",
    "    for folder_path in filelist:\n",
    "        vec = read_attribute_from_all_file(folder_path,time_unit)\n",
    "        feature_vector.append(vec)\n",
    "    return np.vstack(feature_vector)\n",
    "def generate_classifier_feature(feature_vector, n_cluster, time_unit):\n",
    "    k_means = KMeans(n_clusters=n_cluster, random_state=0).fit(feature_vector)\n",
    "    train_classifier = pd.DataFrame()\n",
    "    test_classifier = pd.DataFrame()\n",
    "    for dir in filelist:\n",
    "        train, test = create_feature_for_classifier(k_means, dir, n_cluster, time_unit)\n",
    "        train_classifier = train_classifier.append(train)\n",
    "        test_classifier = test_classifier.append(test)\n",
    "    return train_classifier.append(test_classifier)\n",
    "def create_feature_for_classifier(model, dir, n_cluster, time_unit):\n",
    "    files = os.listdir(dir)\n",
    "    train_per = int(0.67 * len(files))\n",
    "    feature_train = pd.DataFrame()\n",
    "    feature_test = pd.DataFrame()\n",
    "    for file in files[:train_per]: \n",
    "        file_path = os.path.join(dir, file) \n",
    "        data = pd.read_csv(file_path, sep=\" \", index_col=None, names=['x', 'y', 'z'], skip_blank_lines=True).dropna()\n",
    "        segmented_data_train = generate_segments(data,\n",
    "                                                 time_unit)\n",
    "\n",
    "        assignment = vq(segmented_data_train,\n",
    "                        model.cluster_centers_)[0]\n",
    "        assignment_array = np.array(assignment)\n",
    "        feature = [0 for s in\n",
    "                   range(n_cluster + 1)]\n",
    "        for i in assignment_array: \n",
    "            feature[i] += 1\n",
    "        feature[n_cluster] = filelist.index(dir) + 1 \n",
    "        feature_df = pd.DataFrame(np.array(feature).reshape(1, n_cluster + 1))\n",
    "        feature_df.columns = range(1, n_cluster + 2) \n",
    "        feature_train = feature_train.append(feature_df) \n",
    "    for file in files[train_per:]:\n",
    "        file_path = os.path.join(dir, file)\n",
    "        data = pd.read_csv(file_path, sep=\" \", index_col=None, names=['x', 'y', 'z'], skip_blank_lines=True).dropna()\n",
    "        segmented_data_test = generate_segments(data,\n",
    "                                                time_unit)\n",
    "        assignment = vq(segmented_data_test,\n",
    "                        model.cluster_centers_)[0] \n",
    "        assignment_array = np.array(assignment)\n",
    "        feature = [0 for s in\n",
    "                   range(0, n_cluster + 1)] \n",
    "        for i in assignment_array:\n",
    "            feature[i] += 1 \n",
    "        feature[n_cluster] = filelist.index(dir) + 1\n",
    "        feature_df = pd.DataFrame(np.array(feature).reshape(1, n_cluster + 1))\n",
    "        feature_df.columns = range(1, n_cluster + 2)\n",
    "        feature_test = feature_test.append(feature_df)\n",
    "    return feature_train, feature_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy achieved by 3-fold validation is 75.0878989588667%\n",
      "Error rate for the classifier with k value 160 and segment length 16 is 24.912101041133294%\n",
      "Average accuracy achieved by 3-fold validation is 73.78093531319338%\n",
      "Error rate for the classifier with k value 160 and segment length 32 is 26.219064686806615%\n",
      "Average accuracy achieved by 3-fold validation is 74.97269158559482%\n",
      "Error rate for the classifier with k value 320 and segment length 16 is 25.02730841440518%\n",
      "Average accuracy achieved by 3-fold validation is 72.58320532514082%\n",
      "Error rate for the classifier with k value 320 and segment length 32 is 27.41679467485918%\n"
     ]
    }
   ],
   "source": [
    "ks = [160,320]\n",
    "segment_lengths = [16,32]\n",
    "for n_cluster in ks:\n",
    "    for segment_l in segment_lengths :\n",
    "        kf = KFold(3,shuffle = True)\n",
    "        feature_vector = generate_vectors(n_cluster, segment_l)\n",
    "        X = generate_classifier_feature(feature_vector, n_cluster, segment_l)\n",
    "        acc = []\n",
    "        for train_index,test_index in kf.split(X):\n",
    "            random_forest_model = RandomForestClassifier(max_depth=32, random_state=8, n_estimators=90)\n",
    "            random_forest_model.fit(X.iloc[train_index, :n_cluster], X.iloc[train_index, n_cluster])\n",
    "            prediction = random_forest_model.predict(X.iloc[test_index, :n_cluster])\n",
    "            acc.append(accuracy_score(X.iloc[test_index, n_cluster], prediction))\n",
    "        print(\"Average accuracy achieved by 3-fold validation is \" + str(np.mean(acc) * 100) + \"%\")\n",
    "        print(\"Error rate for the classifier with k value \" + str(n_cluster) +  \" and segment length \" + str(segment_l) + \" is \" + str((1-np.mean(acc))*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_clust' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-23b13592850e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mrandom_forest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mbest_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_forest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mbest_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clust\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mcms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_clust' is not defined"
     ]
    }
   ],
   "source": [
    "names = [f for f in os.listdir('data/HMP_Dataset') if (not f.endswith(\".txt\") and (not f.endswith(\".m\")))]\n",
    "best_k = 320\n",
    "best_l = 16\n",
    "kf = KFold(3,shuffle = True)\n",
    "feature_vector = generate_vectors(best_k,best_l)\n",
    "X = generate_classifier_feature(feature_vector,best_k, best_l)\n",
    "cms = []\n",
    "acc = []\n",
    "for train_index,test_index in kf.split(X):\n",
    "    random_forest_model = RandomForestClassifier(max_depth=32, random_state=8, n_estimators=90)\n",
    "    random_forest_model.fit(X.iloc[train_index, :best_k], X.iloc[train_index, best_k])\n",
    "    prediction = random_forest_model.predict(X.iloc[test_index, :best_k])\n",
    "    acc.append(accuracy_score(X.iloc[test_index, n_clust], prediction))\n",
    "    cms.append(confusion_matrix(y_true = X.iloc[test_index, best_k]-1, y_pred = prediction-1,labels=range(14)))\n",
    "plt.figure(figsize=(30,1))\n",
    "ax = plt.subplot(1,1,1, frame_on=False) # no visible frame\n",
    "ax.xaxis.set_visible(False)  # hide the x axis\n",
    "ax.yaxis.set_visible(False)  # hide the y axis\n",
    "pd.plotting.table(ax,data = pd.DataFrame(cms[best_fold_idx],columns= names,index = names))\n",
    "plt.savefig('mytable.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seperate_vectors(n_cluster, time_unit):\n",
    "    feature_vector = []\n",
    "    for folder_path in filelist:\n",
    "        vec = read_attribute_from_all_file(folder_path,time_unit)\n",
    "        feature_vector.append(vec.values)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = generate_vectors(best_k, best_l)\n",
    "seperate_vector = generate_seperate_vectors(best_k,best_l)\n",
    "k_means = KMeans(n_clusters=n_cluster, random_state=0).fit(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,20))\n",
    "for i in range(len(filelist)):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    assignment = vq(seperate_vector[i],\n",
    "                    k_means.cluster_centers_)[0]\n",
    "    fre = np.zeros(best_k)\n",
    "    for j in assignment:\n",
    "        fre[j] +=1\n",
    "    fre = fre / len(os.listdir(filelist[i]))\n",
    "    plt.bar(range(best_k),fre)\n",
    "    plt.hist(assignment,bins = best_k,density=True)\n",
    "    plt.figtext(.5, .9, \"K value is 320\",fontsize = 40)\n",
    "    plt.title(names[i])\n",
    "    plt.xlabel('cluster center')\n",
    "    plt.ylabel('average frequency per file')\n",
    "plt.savefig('histogram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,1))\n",
    "ax = plt.subplot(1,1,1, frame_on=False) # no visible frame\n",
    "ax.xaxis.set_visible(False)  # hide the x axis\n",
    "ax.yaxis.set_visible(False)  # hide the y axis\n",
    "pd.plotting.table(ax,data = pd.DataFrame(cms[best_fold_idx],columns= names,index = names))\n",
    "plt.savefig('mytable.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(['A', 'B', 'A', 'B', 'A', 'B', 'A']).count('A B A B A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\",but ,but\".count(',but')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in range(1,4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
